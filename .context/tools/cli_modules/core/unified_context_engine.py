#!/usr/bin/env python3
"""
üß† Unified Context Engine - –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–ó–∞–º–µ–Ω—è–µ—Ç —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –µ–¥–∏–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –¥–≤–∏–∂–∫–æ–º

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –æ–¥–∏–Ω –º–æ—â–Ω—ã–π –¥–≤–∏–∂–æ–∫:
- AI-powered –∞–Ω–∞–ª–∏–∑ user intent
- Intelligent preloading –Ω–∞ –æ—Å–Ω–æ–≤–µ patterns
- Integration —Å recommend engine
- Fallback –Ω–∞ manual selection

–í–µ—Ä—Å–∏—è: 1.0.0
–°–æ–∑–¥–∞–Ω–æ: 2025-01-15
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
import time

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
try:
    from tools.cli_modules.core.template_manager import TemplateManager
    from tools.cli_modules.commands.ai_commands import IntelligentRecommendationEngine
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ –º–æ–¥—É–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    TemplateManager = None
    IntelligentRecommendationEngine = None


class ContextLoadingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    AUTO_INTELLIGENT = "auto_intelligent"  # AI-powered –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è
    PATTERN_BASED = "pattern_based"        # –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    MANUAL_SELECTION = "manual_selection"  # –†—É—á–Ω–æ–π –≤—ã–±–æ—Ä
    FULL_CONTEXT = "full_context"          # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å—ë


@dataclass
class ContextLoadRequest:
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    user_query: str
    strategy: ContextLoadingStrategy = ContextLoadingStrategy.AUTO_INTELLIGENT
    include_core: bool = True
    include_tasks: bool = True
    max_modules: int = 10
    verbose: bool = False


@dataclass 
class ContextLoadResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    loaded_files: List[str]
    confidence_score: float
    strategy_used: ContextLoadingStrategy
    ai_recommendations: List[Dict]
    fallback_suggestions: List[str]
    execution_time: float
    success: bool
    error_message: Optional[str] = None


class UnifiedContextEngine:
    """–ï–¥–∏–Ω—ã–π –¥–≤–∏–∂–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self, root_path: str = "."):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞
        
        Args:
            root_path: –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –°–õ–ö
        """
        self.root_path = Path(root_path).resolve()
        self.context_path = self.root_path / ".context"
        self.modules_path = self.context_path / "modules"
        self.tasks_path = self.context_path / "tasks"
        
        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
        self.cache = {
            "modules": {},
            "tasks": {},
            "patterns": {},
            "last_update": None
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.context_patterns = {
            "ai_ml": {
                "keywords": ["ai", "ml", "llm", "prompt", "fine-tuning", "agent", "chatbot", "gpt", "claude", "neural", "model", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"],
                "modules": ["ai_ml/prompt_engineering.json", "ai_ml/ai_agent_development.json"],
                "confidence_boost": 0.3
            },
            "python": {
                "keywords": ["python", "django", "fastapi", "pytorch", "tensorflow", "pandas", "pytest", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"],
                "modules": ["languages/python/python_development.json", "core/standards.json"],
                "confidence_boost": 0.2
            },
            "python_dap": {
                "keywords": ["dap", "–±–∏–Ω–¥–∏–Ω–≥", "–±–∏–Ω–¥–∏–Ω–≥–∏", "binding", "bindings", "python-dap", "–ø–∏—Ç–æ–Ω-–¥–∞–ø", "dap-sdk", "cellframe", "—Å–æ–±—ã—Ç–∏—è", "events", "–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è", "crypto"],
                "modules": [
                    "projects/dap_sdk_project.json",
                    "languages/python/python_development.json", 
                    "languages/python/knowledge_base/dap_sdk_binding_standards.json",
                    "core/development_standards.json",
                    "core/standards.json"
                ],
                "confidence_boost": 0.4
            },
            "javascript": {
                "keywords": ["javascript", "react", "vue", "node", "web3", "ethereum", "frontend"],
                "modules": ["languages/javascript/javascript_development.json"],
                "confidence_boost": 0.2
            },
            "crypto": {
                "keywords": ["crypto", "blockchain", "defi", "smart-contract", "post-quantum", "security", "audit"],
                "modules": ["projects/cryptography_project.json", "projects/dap_sdk_project.json"],
                "confidence_boost": 0.3
            },
            "documentation": {
                "keywords": ["documentation", "docs", "markdown", "wiki", "obsidian", "knowledge", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"],
                "modules": ["methodologies/documentation_systems.json", "methodologies/obsidian_workflow.json"],
                "confidence_boost": 0.2
            },
            "architecture": {
                "keywords": ["architecture", "refactoring", "structure", "design", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥"],
                "modules": ["core/standards.json", "core/project.json"],
                "confidence_boost": 0.2
            }
        }
        
        # –í—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏
        self.core_modules = [
            "core/manifest.json",
            "core/standards.json", 
            "core/project.json"
        ]
        
        self.load_cache()
        
    def load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –º–æ–¥—É–ª–µ–π –∏ –∑–∞–¥–∞—á"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥—É–ª—è—Ö
            if self.modules_path.exists():
                for category_dir in self.modules_path.iterdir():
                    if category_dir.is_dir():
                        category = category_dir.name
                        self.cache["modules"][category] = []
                        
                        for module_file in category_dir.rglob("*.json"):
                            rel_path = module_file.relative_to(self.modules_path)
                            self.cache["modules"][category].append(str(rel_path))
                            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–∞—Ö
            if self.tasks_path.exists():
                for task_file in self.tasks_path.glob("*.json"):
                    if not task_file.name.startswith("completed"):
                        self.cache["tasks"][task_file.stem] = str(task_file.relative_to(self.context_path))
                        
            self.cache["last_update"] = datetime.now().isoformat()
            
        except Exception as e:
            print(f"Warning: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à: {e}")
            
    def analyze_user_intent(self, query: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        query_lower = query.lower()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        analysis = {
            "query": query,
            "detected_domains": [],
            "confidence_scores": {},
            "suggested_modules": [],
            "suggested_tasks": [],
            "total_confidence": 0.0
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ–º–µ–Ω–æ–≤
        for domain, config in self.context_patterns.items():
            score = 0.0
            matched_keywords = []
            
            for keyword in config["keywords"]:
                if keyword in query_lower:
                    score += config["confidence_boost"]
                    matched_keywords.append(keyword)
                    
            if score > 0:
                analysis["detected_domains"].append({
                    "domain": domain,
                    "score": score,
                    "matched_keywords": matched_keywords,
                    "suggested_modules": config["modules"]
                })
                analysis["confidence_scores"][domain] = score
                analysis["suggested_modules"].extend(config["modules"])
                
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        analysis["detected_domains"].sort(key=lambda x: x["score"], reverse=True)
        analysis["total_confidence"] = sum(analysis["confidence_scores"].values())
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π
        analysis["suggested_modules"] = list(set(analysis["suggested_modules"]))
        
        return analysis
        
    def find_relevant_tasks(self, query: str) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        relevant_tasks = []
        query_lower = query.lower()
        
        for task_name, task_path in self.cache["tasks"].items():
            full_path = self.context_path / task_path
            
            try:
                if full_path.exists():
                    with open(full_path, 'r', encoding='utf-8') as f:
                        task_data = json.load(f)
                        
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏–∏
                    score = 0.0
                    
                    # –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
                    if any(word in task_name.lower() for word in query_lower.split()):
                        score += 0.5
                        
                    # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
                    if "project" in task_data:
                        project_text = task_data["project"].lower()
                        if any(word in project_text for word in query_lower.split()):
                            score += 0.3
                            
                    # –°—Ç–∞—Ç—É—Å –∏ —Ñ–∞–∑–∞
                    if "status" in task_data:
                        status_text = task_data["status"].lower()
                        if any(word in status_text for word in query_lower.split()):
                            score += 0.2
                            
                    if score > 0:
                        relevant_tasks.append({
                            "name": task_name,
                            "path": task_path,
                            "score": score,
                            "completion": task_data.get("completion", "0%"),
                            "status": task_data.get("status", "unknown")
                        })
                        
            except Exception as e:
                continue
                
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevant_tasks.sort(key=lambda x: x["score"], reverse=True)
        return relevant_tasks[:5]  # –¢–æ–ø 5 –∑–∞–¥–∞—á
        
    def build_context_package(self, analysis: Dict[str, Any], max_modules: int = 6) -> Dict[str, Any]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –ø–∞–∫–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        start_time = time.time()
        
        package = {
            "timestamp": datetime.now().isoformat(),
            "query_analysis": analysis,
            "files_to_load": [],
            "loading_strategy": "automatic",
            "confidence": analysis["total_confidence"],
            "performance": {},
            "processed_files": set()  # –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
        }
        
        # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏
        for core_module in self.core_modules:
            module_path = self.modules_path / core_module
            if module_path.exists():
                package["files_to_load"].append({
                    "path": str(module_path.relative_to(self.root_path)),
                    "type": "core_module",
                    "priority": "high",
                    "reason": "–ë–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å —Å–∏—Å—Ç–µ–º—ã"
                })
                
        # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥—É–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        added_modules = 0
        for domain_info in analysis["detected_domains"]:
            if added_modules >= max_modules - len(self.core_modules):
                break
                
            for module_rel_path in domain_info["suggested_modules"]:
                if added_modules >= max_modules - len(self.core_modules):
                    break
                    
                module_path = self.modules_path / module_rel_path
                if module_path.exists():
                    package["files_to_load"].append({
                        "path": str(module_path.relative_to(self.root_path)),
                        "type": "domain_module",
                        "priority": "medium",
                        "reason": f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥–æ–º–µ–Ω: {domain_info['domain']} (score: {domain_info['score']:.2f})",
                        "matched_keywords": domain_info["matched_keywords"]
                    })
                    added_modules += 1
                    
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏
        relevant_tasks = self.find_relevant_tasks(analysis["query"])
        for task in relevant_tasks[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –∑–∞–¥–∞—á–∏
            task_path = self.context_path / task["path"]
            if task_path.exists():
                package["files_to_load"].append({
                    "path": str(task_path.relative_to(self.root_path)),
                    "type": "task",
                    "priority": "low",
                    "reason": f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞ (score: {task['score']:.2f})",
                    "task_status": task["status"],
                    "completion": task["completion"]
                })
                
        # –î–û–ë–ê–í–õ–ï–ù–û: –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ auto_load —Ñ–∞–π–ª–æ–≤
        self._process_recursive_loading(package)
                
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        end_time = time.time()
        package["performance"] = {
            "analysis_time_ms": round((end_time - start_time) * 1000, 2),
            "total_files": len(package["files_to_load"]),
            "core_modules": len(self.core_modules),
            "domain_modules": added_modules,
            "tasks": len([f for f in package["files_to_load"] if f["type"] == "task"]),
            "recursive_files": len(package.get("processed_files", set()))
        }
        
        return package
        
    def load_context(self, request: 'ContextLoadRequest') -> 'ContextLoadResult':
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        import time
        start_time = time.time()
        
        try:
            query = request.user_query
            
            if request.verbose:
                print(f"üß† –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
                
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            analysis = self.analyze_user_intent(query)
        
            if request.verbose:
                print(f"üéØ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –¥–æ–º–µ–Ω–æ–≤: {len(analysis['detected_domains'])}")
                for domain_info in analysis["detected_domains"]:
                    print(f"   {domain_info['domain']}: {domain_info['score']:.2f} ({', '.join(domain_info['matched_keywords'])})")
                    
            # –°–æ–±–∏—Ä–∞–µ–º –ø–∞–∫–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            package = self.build_context_package(analysis, request.max_modules)
            
            if request.verbose:
                print(f"üì¶ –ü–∞–∫–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–±—Ä–∞–Ω:")
                print(f"   –§–∞–π–ª–æ–≤ –∫ –∑–∞–≥—Ä—É–∑–∫–µ: {package['performance']['total_files']}")
                print(f"   –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {package['performance']['analysis_time_ms']}ms")
                print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {package['confidence']:.2f}")
                
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            loaded_files = [f["path"] for f in package["files_to_load"]]
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            return ContextLoadResult(
                loaded_files=loaded_files,
                confidence_score=package.get('confidence', 0.0),
                strategy_used=request.strategy,
                ai_recommendations=analysis.get('detected_domains', []),
                fallback_suggestions=[],
                execution_time=execution_time,
                success=True
            )
            
        except Exception as e:
            end_time = time.time()
            execution_time = end_time - start_time
            
            return ContextLoadResult(
                loaded_files=[],
                confidence_score=0.0,
                strategy_used=request.strategy,
                ai_recommendations=[],
                fallback_suggestions=[],
                execution_time=execution_time,
                success=False,
                error_message=str(e)
            )
        
    def generate_loading_commands(self, package: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤"""
        commands = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        high_priority = [f for f in package["files_to_load"] if f["priority"] == "high"]
        medium_priority = [f for f in package["files_to_load"] if f["priority"] == "medium"]
        low_priority = [f for f in package["files_to_load"] if f["priority"] == "low"]
        
        # –ö–æ–º–∞–Ω–¥—ã –∑–∞–≥—Ä—É–∑–∫–∏
        for file_info in high_priority + medium_priority + low_priority:
            commands.append(f"# {file_info['reason']}")
            commands.append(f"load_file('{file_info['path']}')")
            
        return commands
        
    def get_fallback_context(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª"""
        return {
            "timestamp": datetime.now().isoformat(),
            "loading_strategy": "fallback",
            "files_to_load": [
                {
                    "path": str((self.modules_path / module).relative_to(self.root_path)),
                    "type": "core_module",
                    "priority": "high",
                    "reason": "–ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–∏—Å—Ç–µ–º—ã"
                }
                for module in self.core_modules
                if (self.modules_path / module).exists()
            ],
            "confidence": 0.0,
            "performance": {
                "analysis_time_ms": 0,
                "total_files": len(self.core_modules)
            }
        }
    
    def get_formatted_context(self, file_paths: List[str], format_type: str = "full") -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        try:
            formatted_context = []
            
            for file_path in file_paths:
                full_path = self.root_path / file_path
                
                if full_path.exists():
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                        if format_type == "full":
                            formatted_context.append(f"=== {file_path} ===")
                            formatted_context.append(content)
                            formatted_context.append("")
                        elif format_type == "summary":
                            # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                            lines = content.split('\n')
                            summary = '\n'.join(lines[:10])  # –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                            formatted_context.append(f"--- {file_path} (–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ) ---")
                            formatted_context.append(summary)
                            if len(lines) > 10:
                                formatted_context.append(f"... –∏ –µ—â–µ {len(lines) - 10} —Å—Ç—Ä–æ–∫")
                            formatted_context.append("")
                            
                    except Exception as e:
                        formatted_context.append(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")
                        
                else:
                    formatted_context.append(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                    
            return '\n'.join(formatted_context)
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}"
            
    def _process_recursive_loading(self, package: Dict[str, Any]) -> None:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç auto_load —Ñ–∞–π–ª—ã –≤ JSON"""
        
        max_depth = 5  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Ä–µ–∫—É—Ä—Å–∏–∏
        current_depth = 0
        files_to_process = list(package["files_to_load"])
        
        while files_to_process and current_depth < max_depth:
            current_depth += 1
            new_files = []
            
            for file_info in files_to_process:
                file_path = self.root_path / file_info["path"]
                file_key = str(file_path)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                if file_key in package["processed_files"]:
                    continue
                    
                package["processed_files"].add(file_key)
                
                try:
                    if file_path.exists() and file_path.suffix == '.json':
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º context_loading_policy.auto_load
                        context = data.get("context", {})
                        loading_policy = context.get("context_loading_policy", {})
                        auto_load_files = loading_policy.get("auto_load", [])
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ auto_load
                        for auto_file in auto_load_files:
                            if isinstance(auto_file, str):
                                auto_file_path = self.modules_path / auto_file
                                if auto_file_path.exists():
                                    new_file_info = {
                                        "path": str(auto_file_path.relative_to(self.root_path)),
                                        "type": "auto_loaded",
                                        "priority": "high",
                                        "reason": f"–ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ {file_info['path']}",
                                        "source_file": file_info["path"]
                                    }
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ —É–∂–µ
                                    if not any(f["path"] == new_file_info["path"] for f in package["files_to_load"]):
                                        package["files_to_load"].append(new_file_info)
                                        new_files.append(new_file_info)
                                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º $ref —Å—Å—ã–ª–∫–∏ (JSON Schema style)
                        self._process_json_refs(data, file_path, package, new_files)
                        
                except Exception as e:
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (–º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ JSON)
                    continue
                    
            files_to_process = new_files
            
    def _process_json_refs(self, data: Any, source_file: Path, package: Dict[str, Any], new_files: List[Dict]) -> None:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç $ref —Å—Å—ã–ª–∫–∏ –≤ JSON"""
        
        if isinstance(data, dict):
            for key, value in data.items():
                if key == "$ref" and isinstance(value, str):
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–Ω–µ URLs)
                    if not value.startswith(('http://', 'https://', '#/')):
                        ref_path = source_file.parent / value
                        if ref_path.exists():
                            new_file_info = {
                                "path": str(ref_path.relative_to(self.root_path)),
                                "type": "referenced",
                                "priority": "medium",
                                "reason": f"$ref —Å—Å—ã–ª–∫–∞ –∏–∑ {source_file.name}",
                                "source_file": str(source_file.relative_to(self.root_path))
                            }
                            
                            if not any(f["path"] == new_file_info["path"] for f in package["files_to_load"]):
                                package["files_to_load"].append(new_file_info)
                                new_files.append(new_file_info)
                                
                elif isinstance(value, (dict, list)):
                    self._process_json_refs(value, source_file, package, new_files)
                    
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, (dict, list)):
                    self._process_json_refs(item, source_file, package, new_files)

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Unified Context Engine - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    parser.add_argument("query", help="–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    parser.add_argument("--max-modules", type=int, default=6, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥—É–ª–µ–π")
    parser.add_argument("--verbose", "-v", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--json", action="store_true", help="JSON –≤—ã–≤–æ–¥")
    parser.add_argument("--commands", action="store_true", help="–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã –∑–∞–≥—Ä—É–∑–∫–∏")
    
    args = parser.parse_args()
    
    engine = UnifiedContextEngine()
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
    request = ContextLoadRequest(
        user_query=args.query,
        max_modules=args.max_modules,
        verbose=args.verbose
    )
    
    result = engine.load_context(request)
    
    if not result.success:
        print(f"‚ùå –û—à–∏–±–∫–∞: {result.error_message}")
        return 1
    
    if args.json:
        output = {
            "loaded_files": result.loaded_files,
            "confidence_score": result.confidence_score,
            "strategy_used": result.strategy_used.value,
            "ai_recommendations": result.ai_recommendations,
            "execution_time": result.execution_time,
            "success": result.success
        }
        print(json.dumps(output, indent=2, ensure_ascii=False))
    else:
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è '{args.query}' –≥–æ—Ç–æ–≤ –∫ –∑–∞–≥—Ä—É–∑–∫–µ")
        print(f"üìä –§–∞–π–ª–æ–≤: {len(result.loaded_files)}, –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence_score:.2f}")
        
        if args.verbose:
            print("\nüìã –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for file_path in result.loaded_files:
                print(f"   ‚Ä¢ {file_path}")
                
    return 0

if __name__ == "__main__":
    main() 