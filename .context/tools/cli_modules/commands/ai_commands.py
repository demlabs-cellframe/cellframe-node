#!/usr/bin/env python3
"""
AI-powered –∫–æ–º–∞–Ω–¥—ã –¥–ª—è Smart Layered Context CLI
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–í–µ—Ä—Å–∏—è: 1.0.0
–°–æ–∑–¥–∞–Ω–æ: 2025-01-15
"""

import os
import json
import math
import re
from collections import defaultdict, Counter
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

from tools.cli_modules.common.base_command import BaseCommand
from tools.cli_modules.core.template_manager import TemplateManager


class TFIDFAnalyzer:
    """TF-IDF –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–æ–¥—É–ª–µ–π"""
    
    def __init__(self, templates_data: Dict[str, Any]):
        self.templates_data = templates_data
        self.documents = self._build_documents()
        self.vocabulary = self._build_vocabulary()
        self.idf_scores = self._calculate_idf()
    
    def _build_documents(self) -> Dict[str, str]:
        """–°—Ç—Ä–æ–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤"""
        documents = {}
        
        for template_path, template_data in self.templates_data.items():
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è, –æ–ø–∏—Å–∞–Ω–∏—è, —Ç–µ–≥–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            text_parts = []
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
            if 'name' in template_data:
                text_parts.append(template_data['name'])
            if 'description' in template_data:
                text_parts.append(template_data['description'])
            
            # –¢–µ–≥–∏
            if 'tags' in template_data:
                text_parts.extend(template_data['tags'])
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ template_info
            if 'template_info' in template_data:
                info = template_data['template_info']
                if 'target_projects' in info:
                    text_parts.extend(info['target_projects'])
                if 'keywords' in info:
                    text_parts.extend(info['keywords'])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –≤ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
            documents[template_path] = ' '.join(text_parts).lower()
        
        return documents
    
    def _build_vocabulary(self) -> set:
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        vocabulary = set()
        
        for doc_text in self.documents.values():
            terms = self._tokenize(doc_text)
            vocabulary.update(terms)
        
        return vocabulary
    
    def _tokenize(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º
        tokens = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø]+\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–∏–ª–∏', '–≤', '–Ω–∞', '–¥–ª—è', '—Å', '–ø–æ', '–æ—Ç', '–∫', 'the', 'a', 'an', 'in', 'on', 'for', 'with', 'to', 'from'}
        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
        
        return tokens
    
    def _calculate_idf(self) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞"""
        idf_scores = {}
        total_docs = len(self.documents)
        
        for term in self.vocabulary:
            docs_with_term = sum(1 for doc_text in self.documents.values() 
                               if term in self._tokenize(doc_text))
            
            if docs_with_term > 0:
                idf_scores[term] = math.log(total_docs / docs_with_term)
            else:
                idf_scores[term] = 0.0
        
        return idf_scores
    
    def calculate_tf_idf_score(self, query: str, template_path: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∏ —à–∞–±–ª–æ–Ω–∞"""
        query_terms = self._tokenize(query)
        doc_text = self.documents.get(template_path, "")
        doc_terms = self._tokenize(doc_text)
        
        if not query_terms or not doc_terms:
            return 0.0
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º TF-IDF –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞
        tf_idf_sum = 0.0
        
        for term in query_terms:
            if term in self.vocabulary:
                # TF - —á–∞—Å—Ç–æ—Ç–∞ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                tf = doc_terms.count(term) / len(doc_terms) if doc_terms else 0
                
                # IDF - –æ–±—Ä–∞—Ç–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                idf = self.idf_scores.get(term, 0)
                
                # TF-IDF
                tf_idf_sum += tf * idf
        
        return tf_idf_sum / len(query_terms) if query_terms else 0.0


class UsageTracker:
    """–¢—Ä–µ–∫–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —à–∞–±–ª–æ–Ω–æ–≤"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.usage_file = self.base_path / ".slc_usage_stats.json"
        self.usage_data = self._load_usage_data()
    
    def _load_usage_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        if self.usage_file.exists():
            try:
                with open(self.usage_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError):
                pass
        
        return {
            "version": "1.0",
            "templates": {},
            "queries": [],
            "sessions": []
        }
    
    def _save_usage_data(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        try:
            with open(self.usage_file, 'w', encoding='utf-8') as f:
                json.dump(self.usage_data, f, ensure_ascii=False, indent=2)
        except IOError as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: {e}")
    
    def track_template_usage(self, template_path: str, action: str = "viewed"):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞"""
        if template_path not in self.usage_data["templates"]:
            self.usage_data["templates"][template_path] = {
                "views": 0,
                "creates": 0,
                "last_used": None
            }
        
        self.usage_data["templates"][template_path][f"{action}s"] += 1
        self.usage_data["templates"][template_path]["last_used"] = self._get_timestamp()
        self._save_usage_data()
    
    def track_query(self, query: str, selected_template: Optional[str] = None):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        query_data = {
            "timestamp": self._get_timestamp(),
            "query": query,
            "selected_template": selected_template
        }
        
        self.usage_data["queries"].append(query_data)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        if len(self.usage_data["queries"]) > 1000:
            self.usage_data["queries"] = self.usage_data["queries"][-1000:]
        
        self._save_usage_data()
    
    def get_usage_score(self, template_path: str) -> float:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —à–∞–±–ª–æ–Ω–∞"""
        template_stats = self.usage_data["templates"].get(template_path, {})
        
        views = template_stats.get("views", 0)
        creates = template_stats.get("creates", 0)
        
        # –°–æ–∑–¥–∞–Ω–∏—è –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å —á–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä—ã
        score = views * 0.3 + creates * 0.7
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ø—Ä–æ—Å—Ç–∞—è)
        max_score = max(
            (stats.get("views", 0) * 0.3 + stats.get("creates", 0) * 0.7)
            for stats in self.usage_data["templates"].values()
        ) if self.usage_data["templates"] else 1
        
        return score / max_score if max_score > 0 else 0.0
    
    def _get_timestamp(self) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π timestamp"""
        from datetime import datetime
        return datetime.now().isoformat()


class IntelligentSemanticAnalyzer:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤
    
    –ó–ê–ú–ï–ù–Ø–ï–¢ keyword-based –ª–æ–≥–∏–∫—É –Ω–∞ –ò–ò-–∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ UniversalSemanticAnalyzer
    """
    
    def __init__(self, ai_manager=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            ai_manager: AI Manager –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        self.ai_manager = ai_manager
        self._analyzer = None
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å UniversalSemanticAnalyzer
        try:
            # AI-–∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è CLI –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—É—Ç–µ–π –∏–º–ø–æ—Ä—Ç–∞
            # CLI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —à–∞–±–ª–æ–Ω–æ–≤
            self.ai_available = False
            self._analyzer = None
            print("‚ÑπÔ∏è CLI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (AI-–∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ API)")
            
        except Exception as e:
            print(f"‚ö†Ô∏è UniversalSemanticAnalyzer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            self.ai_available = False
    
    def analyze_query(self, query: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫—É –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –ò–ò
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        """
        if not self.ai_available or not self._analyzer:
            return self._fallback_analysis(query)
        
        try:
            # –ò–ò-–∞–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ –¥–æ–º–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –æ–±–µ—Ä—Ç–∫–∏
            import asyncio
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º async –∞–Ω–∞–ª–∏–∑ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            loop = None
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            import sys
            import os
            sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../slc-agent/src'))
            from ai.manager.services.universal_semantic_analyzer import AnalysisType
            
            intent_result = loop.run_until_complete(self._analyzer.analyze_single(query, AnalysisType.INTENT))
            domain_result = loop.run_until_complete(self._analyzer.analyze_single(query, AnalysisType.DOMAIN))
            classification_result = loop.run_until_complete(self._analyzer.analyze_single(query, AnalysisType.TASK_CLASSIFICATION))
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç CLI
            intent_analysis = {
                "primary_intent": intent_result.result,
                "confidence": intent_result.confidence,
                "reasoning": intent_result.reasoning
            }
            
            domain_analysis = {
                "primary_domain": domain_result.result,
                "confidence": domain_result.confidence,
                "reasoning": domain_result.reasoning
            }
            
            return {
                "intent": intent_analysis,
                "domain": domain_analysis,
                "task_classification": classification_result.result,
                "original_query": query,
                "analysis_method": "ai_semantic",
                "metadata": {
                    "intent_metadata": intent_result.metadata,
                    "domain_metadata": domain_result.metadata
                }
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ò–ò-–∞–Ω–∞–ª–∏–∑–∞: {e}")
            return self._fallback_analysis(query)
    
    def _fallback_analysis(self, query: str) -> Dict[str, Any]:
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ keyword lists
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ë–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        query_lower = query.lower()
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ hardcoded keywords
        has_action_words = any(word in query_lower for word in ["—Å–æ–∑–¥–∞—Ç—å", "–Ω–∞–π—Ç–∏", "–ø–æ–∫–∞–∑–∞—Ç—å", "help"])
        has_tech_terms = any(word in query_lower for word in ["api", "–∫–æ–¥", "—Ñ—É–Ω–∫—Ü–∏—è", "–∫–ª–∞—Å—Å"])
        has_questions = any(word in query_lower for word in ["—á—Ç–æ", "–∫–∞–∫", "–≥–¥–µ", "why", "what", "how"])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if has_action_words:
            primary_intent = "action_request"
        elif has_questions:
            primary_intent = "information_seeking"
        elif has_tech_terms:
            primary_intent = "technical_inquiry"
        else:
            primary_intent = "general_query"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–º–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if any(word in query_lower for word in ["python", "javascript", "–∫–æ–¥"]):
            primary_domain = "programming"
        elif any(word in query_lower for word in ["ai", "ml", "–º–æ–¥–µ–ª—å"]):
            primary_domain = "ai_ml"
        elif any(word in query_lower for word in ["web", "api", "frontend"]):
            primary_domain = "web_development"
        else:
            primary_domain = "general"
        
        return {
            "intent": {
                "primary_intent": primary_intent,
                "confidence": 0.6,
                "reasoning": "–°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ keyword lists"
            },
            "domain": {
                "primary_domain": primary_domain,
                "confidence": 0.6,
                "reasoning": "–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ hardcoded domains"
            },
            "task_classification": "general",
            "original_query": query,
            "analysis_method": "structural_fallback",
            "metadata": {}
        }


# –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ–Ω–∏–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
SemanticAnalyzer = IntelligentSemanticAnalyzer


class IntelligentRecommendationEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    
    def __init__(self, base_path: str):
        self.base_path = base_path
        self.template_manager = TemplateManager(base_path)
        self.usage_tracker = UsageTracker(base_path)
        self.semantic_analyzer = SemanticAnalyzer()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω–æ–≤
        self.templates_data = self._load_templates_data()
        self.tfidf_analyzer = TFIDFAnalyzer(self.templates_data)
        
        # –í–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
        self.weights = {
            "tfidf_score": 0.40,
            "usage_frequency": 0.30,
            "contextual_relevance": 0.20,
            "semantic_match": 0.10
        }
    
    def _load_templates_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —à–∞–±–ª–æ–Ω–æ–≤"""
        templates_data = {}
        
        # TemplateManager –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∫–∞—Ç–µ–≥–æ—Ä–∏—è -> —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        templates = self.template_manager.list_templates()
        
        for category, template_list in templates.items():
            for template_file in template_list:
                # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: –∫–∞—Ç–µ–≥–æ—Ä–∏—è/—Ñ–∞–π–ª
                template_path = f"{category}/{template_file}"
                
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞
                    full_path = self.template_manager.modules_path / template_path
                    if full_path.exists():
                        with open(full_path, 'r', encoding='utf-8') as f:
                            template_data = json.load(f)
                            templates_data[template_path] = template_data
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —à–∞–±–ª–æ–Ω–∞ {template_path}: {e}")
        
        return templates_data
    
    def get_recommendations(self, query: str, max_results: int = 5, verbose: bool = False) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_analysis = self.semantic_analyzer.analyze_query(query)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–±–ª–æ–Ω–∞
        recommendations = []
        
        for template_path in self.templates_data.keys():
            scores = self._calculate_scores(query, template_path, query_analysis)
            
            final_score = (
                scores["tfidf_score"] * self.weights["tfidf_score"] +
                scores["usage_frequency"] * self.weights["usage_frequency"] +
                scores["contextual_relevance"] * self.weights["contextual_relevance"] +
                scores["semantic_match"] * self.weights["semantic_match"]
            )
            
            if final_score > 0.01:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                recommendations.append({
                    "template": template_path,
                    "score": final_score,
                    "details": scores if verbose else None
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ
        recommendations.sort(key=lambda x: x["score"], reverse=True)
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
        self.usage_tracker.track_query(query)
        
        return recommendations[:max_results]
    
    def _calculate_scores(self, query: str, template_path: str, query_analysis: Dict[str, Any]) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è —à–∞–±–ª–æ–Ω–∞"""
        scores = {}
        
        # TF-IDF –æ—Ü–µ–Ω–∫–∞
        scores["tfidf_score"] = self.tfidf_analyzer.calculate_tf_idf_score(query, template_path)
        
        # –û—Ü–µ–Ω–∫–∞ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        scores["usage_frequency"] = self.usage_tracker.get_usage_score(template_path)
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        scores["contextual_relevance"] = self._calculate_contextual_relevance(
            template_path, query_analysis
        )
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        scores["semantic_match"] = self._calculate_semantic_match(
            template_path, query_analysis
        )
        
        return scores
    
    def _calculate_contextual_relevance(self, template_path: str, query_analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å"""
        template_data = self.templates_data.get(template_path, {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω–∞
        domain_match = 0.0
        detected_domains = query_analysis.get("domain", {})
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –µ—Å–ª–∏ —à–∞–±–ª–æ–Ω –∏–∑ ai_ml, –∞ –∑–∞–ø—Ä–æ—Å –ø—Ä–æ AI
        if "ai_ml" in template_path and "ai_ml" in detected_domains:
            domain_match += 0.5
        
        if "python" in template_path and "python" in detected_domains:
            domain_match += 0.3
        
        if "javascript" in template_path and "javascript" in detected_domains:
            domain_match += 0.3
        
        return min(domain_match, 1.0)
    
    def _calculate_semantic_match(self, template_path: str, query_analysis: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ"""
        template_data = self.templates_data.get(template_path, {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        intent_match = 0.0
        intents = query_analysis.get("intent", {})
        
        # –ï—Å–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ "—Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ–µ–∫—Ç", —Ç–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —à–∞–±–ª–æ–Ω–∞–º
        if "create_project" in intents:
            intent_match += 0.4
        
        # –ï—Å–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ "–Ω–∞–π—Ç–∏ —à–∞–±–ª–æ–Ω", —Ç–æ —Ç–æ–∂–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        if "find_template" in intents:
            intent_match += 0.3
        
        return min(intent_match, 1.0)


class RecommendCommand(BaseCommand):
    """–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    
    def __init__(self, base_path: str):
        super().__init__()
        self.base_path = base_path
        self.recommendation_engine = IntelligentRecommendationEngine(base_path)
    
    @property
    def name(self) -> str:
        return "recommend"
    
    @property
    def description(self) -> str:
        return "üß† –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —à–∞–±–ª–æ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞"
    
    def add_arguments(self, parser):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥—ã"""
        parser.add_argument(
            "query",
            help="–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
        )
        parser.add_argument(
            "--count", "-n",
            type=int,
            default=5,
            help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)"
        )
        parser.add_argument(
            "--verbose", "-v",
            action="store_true",
            help="–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∫–æ—Ä–∏–Ω–≥–µ"
        )
    
    def validate_args(self, args) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥—ã"""
        if not args.query or not args.query.strip():
            print("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
            return False
        
        if args.count < 1 or args.count > 20:
            print("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 1 –¥–æ 20")
            return False
        
        return True
    
    def execute(self, args) -> int:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        try:
            print(f"üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å: '{args.query}'")
            print()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = self.recommendation_engine.get_recommendations(
                args.query,
                max_results=args.count,
                verbose=args.verbose
            )
            
            if not recommendations:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤")
                print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'slc_cli.py search KEYWORD'")
                return 1
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(recommendations)} —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
            print("=" * 60)
            
            for i, rec in enumerate(recommendations, 1):
                template_path = rec["template"]
                score = rec["score"]
                
                # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —à–∞–±–ª–æ–Ω–µ
                try:
                    template_info = self.recommendation_engine.template_manager.get_template_info(template_path)
                    if template_info:
                        name = template_info.get("name", template_path)
                        description = template_info.get("description", "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è")
                    else:
                        name = template_path
                        description = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                except:
                    name = template_path
                    description = "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                
                print(f"{i}. üìÑ {name}")
                print(f"   üìÇ {template_path}")
                print(f"   üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2%}")
                print(f"   üìù {description}")
                
                # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞
                if args.verbose and rec["details"]:
                    details = rec["details"]
                    print(f"   üîç –î–µ—Ç–∞–ª–∏ —Å–∫–æ—Ä–∏–Ω–≥–∞:")
                    print(f"      TF-IDF: {details['tfidf_score']:.3f}")
                    print(f"      –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {details['usage_frequency']:.3f}")
                    print(f"      –ö–æ–Ω—Ç–µ–∫—Å—Ç: {details['contextual_relevance']:.3f}")
                    print(f"      –°–µ–º–∞–Ω—Ç–∏–∫–∞: {details['semantic_match']:.3f}")
                
                print()
            
            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –¥–µ–π—Å—Ç–≤–∏—è
            print("üí° –ß—Ç–æ –¥–∞–ª—å—à–µ:")
            print(f"   slc_cli.py info TEMPLATE_PATH    # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
            print(f"   slc_cli.py create TEMPLATE_PATH PROJECT_NAME    # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–µ–∫—Ç")
            
            return 0
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
            return 1 