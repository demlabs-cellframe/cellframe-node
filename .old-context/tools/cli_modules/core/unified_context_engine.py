#!/usr/bin/env python3
"""
Unified Context Engine - –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –°–õ–ö

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –æ–¥–∏–Ω –º–æ—â–Ω—ã–π –¥–≤–∏–∂–æ–∫:
- AI-powered –∞–Ω–∞–ª–∏–∑ user intent
- Intelligent preloading –Ω–∞ –æ—Å–Ω–æ–≤–µ patterns
- Integration —Å recommend engine
- Fallback –Ω–∞ manual selection

–í–µ—Ä—Å–∏—è: 1.0.0
–°–æ–∑–¥–∞–Ω–æ: 2025-01-15
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
try:
    from tools.cli_modules.core.template_manager import TemplateManager
    from tools.cli_modules.commands.ai_commands import IntelligentRecommendationEngine
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ –º–æ–¥—É–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    TemplateManager = None
    IntelligentRecommendationEngine = None


class ContextLoadingStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    AUTO_INTELLIGENT = "auto_intelligent"  # AI-powered –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è
    PATTERN_BASED = "pattern_based"        # –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    MANUAL_SELECTION = "manual_selection"  # –†—É—á–Ω–æ–π –≤—ã–±–æ—Ä
    FULL_CONTEXT = "full_context"          # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å—ë


@dataclass
class ContextLoadRequest:
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    user_query: str
    strategy: ContextLoadingStrategy = ContextLoadingStrategy.AUTO_INTELLIGENT
    include_core: bool = True
    include_tasks: bool = True
    max_modules: int = 10
    verbose: bool = False


@dataclass 
class ContextLoadResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    loaded_files: List[str]
    confidence_score: float
    strategy_used: ContextLoadingStrategy
    ai_recommendations: List[Dict]
    fallback_suggestions: List[str]
    execution_time: float
    success: bool
    error_message: Optional[str] = None


class UnifiedContextEngine:
    """–ï–¥–∏–Ω—ã–π –¥–≤–∏–∂–æ–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self, base_path: str = "."):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞
        
        Args:
            base_path: –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –°–õ–ö
        """
        self.base_path = Path(base_path)
        self.template_manager = TemplateManager(base_path) if TemplateManager else None
        self.ai_engine = IntelligentRecommendationEngine(base_path) if IntelligentRecommendationEngine else None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã
        self.core_files = self._load_core_files_config()
        self.intent_patterns = self._load_intent_patterns()
        self.domain_keywords = self._load_domain_keywords()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.usage_stats = self._load_usage_stats()
        
    def load_context(self, request: ContextLoadRequest) -> ContextLoadResult:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Args:
            request: –ó–∞–ø—Ä–æ—Å –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Ñ–∞–π–ª–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        import time
        start_time = time.time()
        
        try:
            if request.verbose:
                print(f"üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å: '{request.user_query}'")
                print(f"üìã –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {request.strategy.value}")
            
            # –ê–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
            intent_analysis = self._analyze_user_intent(request.user_query)
            
            # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏
            strategy = self._select_loading_strategy(request, intent_analysis)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            loaded_files = self._execute_loading_strategy(strategy, request, intent_analysis)
            
            # AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            ai_recommendations = self._get_ai_recommendations(request.user_query) if self.ai_engine else []
            
            # –†–∞—Å—á—ë—Ç confidence score
            confidence = self._calculate_confidence_score(intent_analysis, loaded_files, ai_recommendations)
            
            # Fallback –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            fallback_suggestions = self._generate_fallback_suggestions(intent_analysis, loaded_files)
            
            execution_time = time.time() - start_time
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_usage_stats(request, loaded_files, confidence)
            
            result = ContextLoadResult(
                loaded_files=loaded_files,
                confidence_score=confidence,
                strategy_used=strategy,
                ai_recommendations=ai_recommendations,
                fallback_suggestions=fallback_suggestions,
                execution_time=execution_time,
                success=True
            )
            
            if request.verbose:
                self._print_verbose_result(result)
                
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            return ContextLoadResult(
                loaded_files=[],
                confidence_score=0.0,
                strategy_used=strategy if 'strategy' in locals() else request.strategy,
                ai_recommendations=[],
                fallback_suggestions=[],
                execution_time=execution_time,
                success=False,
                error_message=str(e)
            )
    
    def _analyze_user_intent(self, query: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        query_lower = query.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        intent_type = "unknown"
        for pattern_name, patterns in self.intent_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                intent_type = pattern_name
                break
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–µ–Ω–æ–≤
        detected_domains = []
        for domain, keywords in self.domain_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                detected_domains.append(domain)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self._extract_keywords(query)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏
        complexity = self._assess_complexity(query, keywords)
        
        return {
            "intent_type": intent_type,
            "detected_domains": detected_domains,
            "keywords": keywords,
            "complexity": complexity,
            "query_length": len(query),
            "has_technical_terms": self._has_technical_terms(query)
        }
    
    def _select_loading_strategy(self, request: ContextLoadRequest, intent_analysis: Dict) -> ContextLoadingStrategy:
        """–í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        
        # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —è–≤–Ω–æ —É–∫–∞–∑–∞–ª —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        if request.strategy != ContextLoadingStrategy.AUTO_INTELLIGENT:
            return request.strategy
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        confidence_threshold = 0.7
        
        # –ï—Å–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–Ω—è—Ç–Ω–æ –∏ –µ—Å—Ç—å —á—ë—Ç–∫–∏–µ –¥–æ–º–µ–Ω—ã
        if (intent_analysis["intent_type"] != "unknown" and 
            len(intent_analysis["detected_domains"]) > 0 and
            intent_analysis["complexity"] != "high"):
            return ContextLoadingStrategy.AUTO_INTELLIGENT
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–æ –¥–æ–º–µ–Ω –Ω–µ—è—Å–µ–Ω
        if intent_analysis["has_technical_terms"]:
            return ContextLoadingStrategy.PATTERN_BASED
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –∏–ª–∏ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω—ã–π
        if intent_analysis["query_length"] < 10 or intent_analysis["complexity"] == "high":
            return ContextLoadingStrategy.MANUAL_SELECTION
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - intelligent
        return ContextLoadingStrategy.AUTO_INTELLIGENT
    
    def _execute_loading_strategy(self, strategy: ContextLoadingStrategy, 
                                request: ContextLoadRequest, 
                                intent_analysis: Dict) -> List[str]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        
        loaded_files = []
        
        # –í—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º core —Ñ–∞–π–ª—ã –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if request.include_core:
            loaded_files.extend(self._load_core_files())
        
        if strategy == ContextLoadingStrategy.AUTO_INTELLIGENT:
            loaded_files.extend(self._load_intelligent_context(request, intent_analysis))
            
        elif strategy == ContextLoadingStrategy.PATTERN_BASED:
            loaded_files.extend(self._load_pattern_based_context(intent_analysis))
            
        elif strategy == ContextLoadingStrategy.MANUAL_SELECTION:
            loaded_files.extend(self._load_manual_selection_context(request, intent_analysis))
            
        elif strategy == ContextLoadingStrategy.FULL_CONTEXT:
            loaded_files.extend(self._load_full_context())
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if request.include_tasks:
            loaded_files.extend(self._load_active_tasks())
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        loaded_files = self._deduplicate_and_prioritize(loaded_files)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥—É–ª–µ–π
        if len(loaded_files) > request.max_modules:
            loaded_files = loaded_files[:request.max_modules]
        
        return loaded_files
    
    def _load_intelligent_context(self, request: ContextLoadRequest, intent_analysis: Dict) -> List[str]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ AI –∞–Ω–∞–ª–∏–∑–∞"""
        files = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI engine –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.ai_engine:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É –º–µ—Ç–æ–¥–∞
                import inspect
                sig = inspect.signature(self.ai_engine.get_recommendations)
                if 'limit' in sig.parameters:
                    ai_recommendations = self.ai_engine.get_recommendations(request.user_query, limit=5)
                else:
                    ai_recommendations = self.ai_engine.get_recommendations(request.user_query, verbose=False)[:5]
                
                for rec in ai_recommendations:
                    if "template_path" in rec:
                        file_path = f"modules/{rec['template_path']}"
                        if self._file_exists(file_path):
                            files.append(file_path)
            except Exception as e:
                print(f"‚ö†Ô∏è  AI engine error: {e}")
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–µ–Ω–æ–≤
        for domain in intent_analysis["detected_domains"]:
            domain_files = self._get_domain_files(domain)
            files.extend(domain_files[:2])  # –ú–∞–∫—Å–∏–º—É–º 2 —Ñ–∞–π–ª–∞ –Ω–∞ –¥–æ–º–µ–Ω
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        intent_files = self._get_intent_files(intent_analysis["intent_type"])
        files.extend(intent_files)
        
        return files
    
    def _load_pattern_based_context(self, intent_analysis: Dict) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –ø—Ä–∞–≤–∏–ª"""
        files = []
        
        # –§–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in intent_analysis["keywords"][:5]:  # –¢–æ–ø 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_files = self._find_files_by_keyword(keyword)
            files.extend(keyword_files[:2])
        
        # –§–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–µ–Ω–æ–≤
        for domain in intent_analysis["detected_domains"]:
            domain_files = self._get_domain_files(domain)
            files.extend(domain_files)
        
        return files
    
    def _load_manual_selection_context(self, request: ContextLoadRequest, intent_analysis: Dict) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Ä—É—á–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞"""
        files = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä
        if intent_analysis["detected_domains"]:
            # –û–¥–∏–Ω —Ñ–∞–π–ª –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
            for domain in intent_analysis["detected_domains"][:3]:
                domain_files = self._get_domain_files(domain)
                if domain_files:
                    files.append(domain_files[0])
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥—É–ª–∏
            popular_files = self._get_popular_modules(3)
            files.extend(popular_files)
        
        return files
    
    def _load_full_context(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–≤—Å–µ –º–æ–¥—É–ª–∏)"""
        files = []
        
        # –í—Å–µ –º–æ–¥—É–ª–∏ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ modules/
        modules_path = self.base_path / "modules"
        if modules_path.exists():
            for category_dir in modules_path.iterdir():
                if category_dir.is_dir():
                    for json_file in category_dir.glob("**/*.json"):
                        relative_path = json_file.relative_to(self.base_path)
                        files.append(str(relative_path))
        
        return files
    
    def _load_core_files_config(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é core —Ñ–∞–π–ª–æ–≤"""
        return [
            "manifest.json",
            "modules/core/standards.json", 
            "modules/core/project.json"
        ]
    
    def _load_core_files(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç core —Ñ–∞–π–ª—ã —Å–∏—Å—Ç–µ–º—ã"""
        files = []
        for file_path in self.core_files:
            if self._file_exists(file_path):
                files.append(file_path)
        return files
    
    def _load_active_tasks(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏"""
        task_files = [
            "tasks/active.json",
            "tasks/analysis/slc_v4_comprehensive_analysis.json"
        ]
        
        return [f for f in task_files if self._file_exists(f)]
    
    def _get_ai_recommendations(self, query: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç AI engine"""
        if not self.ai_engine:
            return []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É –º–µ—Ç–æ–¥–∞ get_recommendations
            import inspect
            sig = inspect.signature(self.ai_engine.get_recommendations)
            if 'limit' in sig.parameters:
                return self.ai_engine.get_recommendations(query, limit=3, verbose=False)
            else:
                # Fallback –¥–ª—è —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
                recommendations = self.ai_engine.get_recommendations(query, verbose=False)
                return recommendations[:3] if recommendations else []
        except Exception as e:
            print(f"‚ö†Ô∏è  AI recommendations error: {e}")
            return []
    
    def _load_intent_patterns(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–º–µ—Ä–µ–Ω–∏–π"""
        return {
            "create_project": ["—Å–æ–∑–¥–∞—Ç—å", "–Ω–æ–≤—ã–π –ø—Ä–æ–µ–∫—Ç", "template", "generate", "start"],
            "find_information": ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "info", "explain"],
            "solve_problem": ["–æ—à–∏–±–∫–∞", "–ø—Ä–æ–±–ª–µ–º–∞", "fix", "debug", "–ø–æ–º–æ—â—å", "issue"],
            "analyze_technology": ["–∞–Ω–∞–ª–∏–∑", "audit", "review", "–æ—Ü–µ–Ω–∫–∞", "research"],
            "learn_domain": ["–∏–∑—É—á–∏—Ç—å", "learn", "tutorial", "guide", "–Ω–∞—á–∞–ª–æ"],
            "optimize_performance": ["–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "performance", "speed", "optimize", "improve"]
        }
    
    def _load_domain_keywords(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–º–µ–Ω–æ–≤"""
        return {
            "ai_ml": ["ai", "ml", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "llm", "gpt", "prompt"],
            "python": ["python", "django", "fastapi", "flask", "pandas", "numpy"],
            "javascript": ["javascript", "js", "react", "vue", "node", "npm", "web"],
            "crypto": ["–∫—Ä–∏–ø—Ç–æ–≥—Ä–∞—Ñ–∏—è", "blockchain", "defi", "smart contracts", "security"],
            "methodologies": ["–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è", "–ø—Ä–æ—Ü–µ—Å—Å", "workflow", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã", "best practices"],
            "tools": ["–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "ci/cd", "deployment", "monitoring"],
            "documentation": ["–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "markdown", "wiki", "guide", "readme"]
        }
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = re.findall(r'\w+', query.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∫–∞–∫', '—á—Ç–æ', 'the', 'a', 'an', 'and', 'or', 'but'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _assess_complexity(self, query: str, keywords: List[str]) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        if len(query) > 100 or len(keywords) > 8:
            return "high"
        elif len(query) < 20 or len(keywords) < 3:
            return "low"
        else:
            return "medium"
    
    def _has_technical_terms(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        technical_terms = [
            'api', 'json', 'database', 'algorithm', 'framework', 'library',
            'api', 'json', '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ñ—Ä–µ–π–º–≤–æ—Ä–∫', '–±–∏–±–ª–∏–æ—Ç–µ–∫–∞'
        ]
        query_lower = query.lower()
        return any(term in query_lower for term in technical_terms)
    
    def _get_domain_files(self, domain: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞"""
        domain_mapping = {
            "ai_ml": ["modules/ai_ml/prompt_engineering.json", "modules/ai_ml/fine_tuning_workflow.json"],
            "python": ["modules/languages/python/python_development.json"],
            "javascript": ["modules/languages/javascript/javascript_development.json"],
            "crypto": ["modules/projects/cryptography_project.json", "modules/methodologies/defi_security_audit.json"],
            "methodologies": ["modules/methodologies/obsidian_workflow.json", "modules/methodologies/documentation_systems.json"],
            "tools": ["modules/tools/slc_analytics.json", "modules/tools/slc_evolution_export.json"]
        }
        
        domain_files = domain_mapping.get(domain, [])
        return [f for f in domain_files if self._file_exists(f)]
    
    def _get_intent_files(self, intent_type: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        intent_mapping = {
            "create_project": ["tasks/templates/"],
            "solve_problem": ["modules/tools/", "docs/"],
            "analyze_technology": ["modules/methodologies/"],
            "optimize_performance": ["modules/tools/slc_analytics.json"]
        }
        
        return intent_mapping.get(intent_type, [])
    
    def _find_files_by_keyword(self, keyword: str) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É"""
        files = []
        
        # –ü–æ–∏—Å–∫ –≤ template manager –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.template_manager:
            try:
                search_results = self.template_manager.search_templates(keyword)
                for category, template_list in search_results.items():
                    for template in template_list:
                        files.append(f"modules/{category}/{template}")
            except Exception:
                pass
        
        return files
    
    def _get_popular_modules(self, limit: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥—É–ª–∏"""
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
        popular = [
            "modules/ai_ml/prompt_engineering.json",
            "modules/languages/python/python_development.json",
            "modules/methodologies/obsidian_workflow.json"
        ]
        
        return [f for f in popular[:limit] if self._file_exists(f)]
    
    def _file_exists(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        return (self.base_path / file_path).exists()
    
    def _deduplicate_and_prioritize(self, files: List[str]) -> List[str]:
        """–£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É"""
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_files = list(dict.fromkeys(files))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (core > modules > tasks > tools)
        def priority_key(file_path: str) -> int:
            if file_path.startswith("modules/core/"):
                return 0
            elif file_path.startswith("modules/"):
                return 1
            elif file_path.startswith("tasks/"):
                return 2
            else:
                return 3
        
        return sorted(unique_files, key=priority_key)
    
    def _calculate_confidence_score(self, intent_analysis: Dict, loaded_files: List[str], ai_recommendations: List[Dict]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç confidence score –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        score = 0.0
        
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        if intent_analysis["intent_type"] != "unknown":
            score += 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        if intent_analysis["detected_domains"]:
            score += 0.2 * min(len(intent_analysis["detected_domains"]), 3)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if ai_recommendations:
            score += 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if loaded_files:
            score += min(0.2, len(loaded_files) * 0.02)
        
        return min(score, 1.0)
    
    def _generate_fallback_suggestions(self, intent_analysis: Dict, loaded_files: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è fallback'–∞"""
        suggestions = []
        
        if not loaded_files:
            suggestions.append("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å '–∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç'")
            suggestions.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
        
        if not intent_analysis["detected_domains"]:
            suggestions.append("–£–∫–∞–∂–∏—Ç–µ –æ–±–ª–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã (AI, Python, crypto, etc.)")
        
        return suggestions
    
    def _load_usage_stats(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        stats_file = self.base_path / ".slc" / ".slc_usage_stats.json"
        if stats_file.exists():
            try:
                with open(stats_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {}
    
    def _update_usage_stats(self, request: ContextLoadRequest, loaded_files: List[str], confidence: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
        pass
    
    def load_files_content(self, file_paths: List[str]) -> Dict[str, str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å {–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ}
        """
        content = {}
        
        for file_path in file_paths:
            try:
                full_path = self.base_path / file_path
                if full_path.exists():
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content[file_path] = f.read()
                else:
                    content[file_path] = f"# –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}"
            except Exception as e:
                content[file_path] = f"# –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}"
        
        return content
    
    def get_formatted_context(self, file_paths: List[str], format_type: str = "full") -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≥–æ—Ç–æ–≤—ã–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ò–ò
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            format_type: –¢–∏–ø —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (full, compact, structured)
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        content = self.load_files_content(file_paths)
        
        if format_type == "compact":
            return self._format_compact_context(content)
        elif format_type == "structured":
            return self._format_structured_context(content)
        else:
            return self._format_full_context(content)
    
    def _format_full_context(self, content: Dict[str, str]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏"""
        output = []
        output.append("=" * 80)
        output.append("SMART LAYERED CONTEXT - –ü–û–õ–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø –ò–ò")
        output.append("=" * 80)
        output.append("")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = {
            "üìã CORE –°–ò–°–¢–ï–ú–ê": [],
            "üß† –ú–û–î–£–õ–ò": [],
            "üìù –ó–ê–î–ê–ß–ò": [],
            "üõ†Ô∏è –ò–ù–°–¢–†–£–ú–ï–ù–¢–´": []
        }
        
        for file_path, file_content in content.items():
            if file_path.startswith("modules/core/"):
                categories["üìã CORE –°–ò–°–¢–ï–ú–ê"].append((file_path, file_content))
            elif file_path.startswith("modules/"):
                categories["üß† –ú–û–î–£–õ–ò"].append((file_path, file_content))
            elif file_path.startswith("tasks/"):
                categories["üìù –ó–ê–î–ê–ß–ò"].append((file_path, file_content))
            else:
                categories["üõ†Ô∏è –ò–ù–°–¢–†–£–ú–ï–ù–¢–´"].append((file_path, file_content))
        
        # –í—ã–≤–æ–¥–∏–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category_name, files in categories.items():
            if files:
                output.append(f"\n{category_name}")
                output.append("-" * 60)
                
                for file_path, file_content in files:
                    output.append(f"\nüìÑ –§–ê–ô–õ: {file_path}")
                    output.append("```")
                    output.append(file_content)
                    output.append("```")
                    output.append("")
        
        output.append("=" * 80)
        output.append(f"‚úÖ –ö–û–ù–¢–ï–ö–°–¢ –ó–ê–ì–†–£–ñ–ï–ù: {len(content)} —Ñ–∞–π–ª–æ–≤")
        output.append("=" * 80)
        
        return "\n".join(output)
    
    def _format_compact_context(self, content: Dict[str, str]) -> str:
        """–ö–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π"""
        output = []
        
        for file_path, file_content in content.items():
            output.append(f"# {file_path}")
            output.append(file_content)
            output.append("")
        
        return "\n".join(output)
    
    def _format_structured_context(self, content: Dict[str, str]) -> str:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å JSON"""
        structured = {
            "context_metadata": {
                "loaded_files": list(content.keys()),
                "total_files": len(content),
                "timestamp": "2025-01-15T23:45:00Z"
            },
            "file_contents": content
        }
        
        return json.dumps(structured, indent=2, ensure_ascii=False)
    
    def _print_verbose_result(self, result: ContextLoadResult):
        """–í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ"""
        print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
        print(f"   üìä Confidence Score: {result.confidence_score:.2f}")
        print(f"   ‚ö° –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result.execution_time:.3f}—Å")
        print(f"   üìã –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {result.strategy_used.value}")
        print(f"   üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(result.loaded_files)}")
        
        if result.loaded_files:
            print("   üìÑ –§–∞–π–ª—ã:")
            for file_path in result.loaded_files:
                print(f"      ‚Ä¢ {file_path}")
        
        if result.ai_recommendations:
            print(f"   ü§ñ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {len(result.ai_recommendations)}")
        
        if result.fallback_suggestions:
            print("   üí° –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:")
            for suggestion in result.fallback_suggestions:
                print(f"      ‚Ä¢ {suggestion}")


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def quick_load_context(query: str, verbose: bool = False) -> ContextLoadResult:
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
    engine = UnifiedContextEngine()
    request = ContextLoadRequest(user_query=query, verbose=verbose)
    return engine.load_context(request)


def load_full_context(verbose: bool = False) -> ContextLoadResult:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    engine = UnifiedContextEngine()
    request = ContextLoadRequest(
        user_query="load everything",
        strategy=ContextLoadingStrategy.FULL_CONTEXT,
        verbose=verbose
    )
    return engine.load_context(request)


if __name__ == "__main__":
    # –ü—Ä–æ—Å—Ç–æ–π CLI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python unified_context_engine.py '–≤–∞—à –∑–∞–ø—Ä–æ—Å'")
        sys.exit(1)
    
    query = " ".join(sys.argv[1:])
    result = quick_load_context(query, verbose=True)
    
    if result.success:
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(result.loaded_files)} —Ñ–∞–π–ª–æ–≤")
    else:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {result.error_message}") 