#!/usr/bin/env python3
"""
ML Categorization System
–°–∏—Å—Ç–µ–º–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π Cellframe API
–§–∞–∑–∞ 3 –ø—Ä–æ–µ–∫—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import joblib
import datetime

class MLCategorizationSystem:
    """–°–∏—Å—Ç–µ–º–∞ ML –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–π API"""
    
    def __init__(self, model_dir: str = ".context/ml-models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        # –ú–æ–¥–µ–ª–∏
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3),
            lowercase=True
        )
        self.classifier = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Å–∞–º–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        self.categories = {
            'critical_core': {
                'weight': 100,
                'keywords': [
                    'init', 'deinit', 'create', 'destroy', 'alloc', 'free',
                    'start', 'stop', 'main', 'core', 'system', 'global'
                ],
                'patterns': [
                    r'.*_init$', r'.*_deinit$', r'.*_create$', r'.*_destroy$',
                    r'^dap_common_.*', r'^dap_config_.*', r'^dap_proc_.*'
                ]
            },
            'blockchain_operations': {
                'weight': 95,
                'keywords': [
                    'chain', 'block', 'ledger', 'transaction', 'tx', 'consensus',
                    'mining', 'stake', 'validator', 'node', 'sync'
                ],
                'patterns': [
                    r'^dap_chain_.*', r'^dap_ledger_.*', r'^dap_consensus_.*',
                    r'.*_block_.*', r'.*_tx_.*', r'.*_transaction_.*'
                ]
            },
            'network_layer': {
                'weight': 85,
                'keywords': [
                    'net', 'network', 'stream', 'client', 'server', 'http',
                    'socket', 'connection', 'peer', 'protocol', 'packet'
                ],
                'patterns': [
                    r'^dap_stream_.*', r'^dap_client_.*', r'^dap_server_.*',
                    r'^dap_http_.*', r'.*_net_.*', r'.*_peer_.*'
                ]
            },
            'cryptography': {
                'weight': 90,
                'keywords': [
                    'crypto', 'hash', 'sign', 'verify', 'encrypt', 'decrypt',
                    'key', 'cert', 'pkey', 'signature', 'cipher'
                ],
                'patterns': [
                    r'^dap_enc_.*', r'^dap_hash_.*', r'^dap_sign_.*',
                    r'^dap_crypto_.*', r'.*_key_.*', r'.*_cert_.*'
                ]
            },
            'data_structures': {
                'weight': 75,
                'keywords': [
                    'list', 'hash_table', 'tree', 'queue', 'stack', 'array',
                    'buffer', 'string', 'memory', 'data'
                ],
                'patterns': [
                    r'^dap_list_.*', r'^dap_hash_table_.*', r'^dap_tree_.*',
                    r'^dap_string_.*', r'.*_list_.*', r'.*_array_.*'
                ]
            },
            'python_integration': {
                'weight': 80,
                'keywords': [
                    'python', 'py', 'init', 'module', 'object', 'method',
                    'wrapper', 'binding', 'api'
                ],
                'patterns': [
                    r'^PyInit_.*', r'^py_.*', r'.*_python_.*',
                    r'.*_py_.*', r'.*_wrapper_.*'
                ]
            },
            'utilities': {
                'weight': 60,
                'keywords': [
                    'util', 'helper', 'tool', 'format', 'convert', 'parse',
                    'validate', 'check', 'get', 'set', 'find'
                ],
                'patterns': [
                    r'^dap_.*_get$', r'^dap_.*_set$', r'^dap_.*_find$',
                    r'.*_util_.*', r'.*_helper_.*', r'.*_tool_.*'
                ]
            },
            'testing_debug': {
                'weight': 30,
                'keywords': [
                    'test', 'debug', 'mock', 'stub', 'trace', 'log',
                    'print', 'dump', 'check', 'assert'
                ],
                'patterns': [
                    r'.*test.*', r'.*debug.*', r'.*mock.*',
                    r'.*_print$', r'.*_dump$', r'.*_trace$'
                ]
            }
        }
        
        # –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.is_trained = False
        self.training_accuracy = 0.0
        self.feature_importance = {}

    def extract_features(self, function_data: Dict) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–∏"""
        features = []
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
        name = function_data.get('name', '')
        features.append(name)
        
        # –°–∏–≥–Ω–∞—Ç—É—Ä–∞
        signature = function_data.get('signature', '')
        features.append(signature)
        
        # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        comments = ' '.join(function_data.get('comments', []))
        features.append(comments)
        
        # –ú–æ–¥—É–ª—å
        module = function_data.get('module', '')
        features.append(module)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        params = function_data.get('parameters', [])
        param_types = ' '.join([p.get('type', '') for p in params])
        features.append(param_types)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Ç–∏–ø
        return_type = function_data.get('return_type', '')
        features.append(return_type)
        
        return ' '.join(features).lower()

    def rule_based_categorization(self, function_data: Dict) -> Tuple[str, float]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª"""
        name = function_data.get('name', '').lower()
        module = function_data.get('module', '').lower()
        signature = function_data.get('signature', '').lower()
        
        best_category = 'utilities'
        best_score = 0.0
        
        for category, config in self.categories.items():
            score = 0.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in config['patterns']:
                if re.match(pattern, name, re.IGNORECASE):
                    score += config['weight'] * 0.8
                    break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keyword_matches = 0
            for keyword in config['keywords']:
                if keyword in name or keyword in module or keyword in signature:
                    keyword_matches += 1
            
            if keyword_matches > 0:
                score += (keyword_matches / len(config['keywords'])) * config['weight'] * 0.6
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫—É
            score = min(score, 100.0)
            
            if score > best_score:
                best_score = score
                best_category = category
        
        return best_category, best_score / 100.0

    def prepare_training_data(self, api_data: Dict) -> Tuple[List[str], List[str]]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        features = []
        labels = []
        
        for module_name, module_data in api_data.get('modules', {}).items():
            for function in module_data.get('functions', []):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                feature_text = self.extract_features(function)
                features.append(feature_text)
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é (–∏—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∫–∞–∫ ground truth)
                category, confidence = self.rule_based_categorization(function)
                labels.append(category)
        
        return features, labels

    def train_model(self, api_data: Dict) -> Dict:
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        print("ü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features, labels = self.prepare_training_data(api_data)
        
        if len(features) < 10:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 —Ñ—É–Ω–∫—Ü–∏–π)")
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        X = self.vectorizer.fit_transform(features)
        y = np.array(labels)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.classifier.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        y_pred = self.classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_names = self.vectorizer.get_feature_names_out()
        importance_scores = self.classifier.feature_importances_
        
        # –¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        top_indices = np.argsort(importance_scores)[-20:]
        self.feature_importance = {
            feature_names[i]: float(importance_scores[i]) 
            for i in top_indices
        }
        
        self.is_trained = True
        self.training_accuracy = accuracy
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.save_models()
        
        training_report = {
            'accuracy': accuracy,
            'classification_report': report,
            'feature_importance': self.feature_importance,
            'training_samples': len(features),
            'categories': list(self.categories.keys()),
            'timestamp': datetime.datetime.now().isoformat()
        }
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é: {accuracy:.3f}")
        return training_report

    def predict_category(self, function_data: Dict) -> Tuple[str, float, Dict]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ñ—É–Ω–∫—Ü–∏–∏"""
        if not self.is_trained:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –ø–æ–¥—Ö–æ–¥
            category, confidence = self.rule_based_categorization(function_data)
            return category, confidence, {'method': 'rule_based'}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_text = self.extract_features(function_data)
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        X = self.vectorizer.transform([feature_text])
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.classifier.predict(X)[0]
        probabilities = self.classifier.predict_proba(X)[0]
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories = self.classifier.classes_
        pred_index = np.where(categories == prediction)[0][0]
        confidence = probabilities[pred_index]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        details = {
            'method': 'ml_model',
            'all_probabilities': {
                cat: float(prob) for cat, prob in zip(categories, probabilities)
            },
            'top_features': self.get_top_features_for_prediction(feature_text)
        }
        
        return prediction, confidence, details

    def get_top_features_for_prediction(self, feature_text: str, top_n: int = 5) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            X = self.vectorizer.transform([feature_text])
            feature_names = self.vectorizer.get_feature_names_out()
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–Ω—É–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            nonzero_indices = X.nonzero()[1]
            feature_scores = [(feature_names[i], X[0, i]) for i in nonzero_indices]
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            feature_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [feature for feature, score in feature_scores[:top_n]]
        except:
            return []

    def categorize_functions_batch(self, functions: List[Dict]) -> List[Dict]:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç —Ñ—É–Ω–∫—Ü–∏–π"""
        results = []
        
        for function in functions:
            category, confidence, details = self.predict_category(function)
            
            result = {
                'function': function,
                'predicted_category': category,
                'confidence': confidence,
                'prediction_details': details
            }
            
            results.append(result)
        
        return results

    def save_models(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
            joblib.dump(self.vectorizer, self.model_dir / 'vectorizer.pkl')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            joblib.dump(self.classifier, self.model_dir / 'classifier.pkl')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'is_trained': self.is_trained,
                'training_accuracy': self.training_accuracy,
                'feature_importance': self.feature_importance,
                'categories': list(self.categories.keys()),
                'saved_at': datetime.datetime.now().isoformat()
            }
            
            with open(self.model_dir / 'metadata.json', 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.model_dir}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")

    def load_models(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
            self.vectorizer = joblib.load(self.model_dir / 'vectorizer.pkl')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            self.classifier = joblib.load(self.model_dir / 'classifier.pkl')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(self.model_dir / 'metadata.json', 'r') as f:
                metadata = json.load(f)
            
            self.is_trained = metadata.get('is_trained', False)
            self.training_accuracy = metadata.get('training_accuracy', 0.0)
            self.feature_importance = metadata.get('feature_importance', {})
            
            print(f"üì¶ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—Ç–æ—á–Ω–æ—Å—Ç—å: {self.training_accuracy:.3f})")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏: {e}")
            return False

    def evaluate_categorization_quality(self, test_functions: List[Dict]) -> Dict:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        results = self.categorize_functions_batch(test_functions)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = {}
        confidence_scores = []
        
        for result in results:
            category = result['predicted_category']
            confidence = result['confidence']
            
            if category not in category_stats:
                category_stats[category] = {
                    'count': 0,
                    'avg_confidence': 0.0,
                    'confidences': []
                }
            
            category_stats[category]['count'] += 1
            category_stats[category]['confidences'].append(confidence)
            confidence_scores.append(confidence)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for category in category_stats:
            confidences = category_stats[category]['confidences']
            category_stats[category]['avg_confidence'] = np.mean(confidences)
            category_stats[category]['std_confidence'] = np.std(confidences)
        
        evaluation = {
            'total_functions': len(test_functions),
            'avg_confidence': np.mean(confidence_scores),
            'std_confidence': np.std(confidence_scores),
            'category_distribution': category_stats,
            'high_confidence_ratio': len([c for c in confidence_scores if c > 0.8]) / len(confidence_scores),
            'low_confidence_ratio': len([c for c in confidence_scores if c < 0.5]) / len(confidence_scores),
            'model_accuracy': self.training_accuracy,
            'evaluation_timestamp': datetime.datetime.now().isoformat()
        }
        
        return evaluation

    def generate_categorization_report(self, functions: List[Dict]) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        results = self.categorize_functions_batch(functions)
        evaluation = self.evaluate_categorization_quality(functions)
        
        report = {
            'summary': {
                'total_functions_categorized': len(functions),
                'model_used': 'ml_model' if self.is_trained else 'rule_based',
                'average_confidence': evaluation['avg_confidence'],
                'high_confidence_predictions': evaluation['high_confidence_ratio'] * 100
            },
            'category_breakdown': evaluation['category_distribution'],
            'quality_metrics': {
                'model_accuracy': self.training_accuracy,
                'prediction_confidence': evaluation['avg_confidence'],
                'confidence_std': evaluation['std_confidence']
            },
            'recommendations': self.generate_improvement_recommendations(evaluation),
            'detailed_results': results[:10],  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            'timestamp': datetime.datetime.now().isoformat()
        }
        
        return report

    def generate_improvement_recommendations(self, evaluation: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        
        if evaluation['avg_confidence'] < 0.7:
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        
        if evaluation['low_confidence_ratio'] > 0.2:
            recommendations.append("–£–ª—É—á—à–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é")
        
        if self.training_accuracy < 0.85:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_counts = [stats['count'] for stats in evaluation['category_distribution'].values()]
        if max(category_counts) / min(category_counts) > 10:
            recommendations.append("–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
        
        return recommendations

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ML Categorization System')
    parser.add_argument('--api-data', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ API')
    parser.add_argument('--train', action='store_true', help='–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å')
    parser.add_argument('--evaluate', action='store_true', help='–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ')
    parser.add_argument('--output-report', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    ml_system = MLCategorizationSystem()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ API
    with open(args.api_data, 'r', encoding='utf-8') as f:
        api_data = json.load(f)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏
    ml_system.load_models()
    
    if args.train:
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        training_report = ml_system.train_model(api_data)
        print("üìä –û—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏:")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {training_report['accuracy']:.3f}")
        print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {training_report['training_samples']}")
    
    if args.evaluate:
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        all_functions = []
        for module_data in api_data.get('modules', {}).values():
            all_functions.extend(module_data.get('functions', []))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = ml_system.generate_categorization_report(all_functions[:100])  # –ü–µ—Ä–≤—ã–µ 100
        
        print("üìà –û—Ç—á–µ—Ç –æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏:")
        print(f"   –§—É–Ω–∫—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {report['summary']['total_functions_categorized']}")
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['summary']['average_confidence']:.3f}")
        print(f"   –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {report['summary']['high_confidence_predictions']:.1f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if args.output_report:
            with open(args.output_report, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {args.output_report}")

if __name__ == '__main__':
    main() 