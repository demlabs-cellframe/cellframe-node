# Stage Environment Configuration
# Default values for stage-env system
# Can be overridden in scenarios or via environment variables

[network]
# Network identity
name = stagenet
network_id = 0x1234
consensus_type = esbocs

[topology]
# Default node counts
root_nodes_count = 3
master_nodes_count = 3
full_nodes_count = 1

[build]
# Build configuration
build_type = debug
cellframe_version = latest

[network_settings]
# Network ports and addressing
base_rpc_port = 8545
base_p2p_port = 31337
base_cf_port = 7007
base_http_port = 8079
node_port = 8079
base_ip = 172.20.0.10
subnet = 172.20.0.0/16

[consensus]
# Consensus parameters
min_validators = 2
new_round_delay = 45
collecting_level = 10.0
auth_certs_prefix = stagenet.master

[balancer]
# Network balancer settings
enabled = true
type = http
uri = f0intlt4eyl03htogu
max_links_response = 10
request_delay = 20

[features]
# Optional features
monitoring = false
tests = false
crash_artifacts = true

[logging]
# Logging configuration
log_dir = ../testing/logs
log_level = info
scenario_logs = true
retain_days = 7

[paths]
# Directory paths (relative to stage-env/)
cache_dir = ../testing/cache
artifacts_dir = ../testing/artifacts

[artifacts]
# Artifacts collection settings
collect_node_logs = true
collect_health_logs = true
collect_crash_dumps = true
retain_days = 30

[timeouts]
# Operation timeouts in seconds
startup = 600
health_check = 600
command = 30

[scenarios]
debug = false

[node_source]
# ============================================================================
# Cellframe Node Package Source Configuration
# ============================================================================
# Defines where to get the cellframe-node package for Docker containers
# 
# Supported types:
#   - url        : Download .deb package from HTTP/HTTPS URL
#   - repository : Build from git repository (source build)
#   - local      : Use local .deb file from filesystem
#
# Choose ONE type and configure its parameters below
# ============================================================================

# ----------------------------------------------------------------------------
# Option 1: URL - Download pre-built package from web
# ----------------------------------------------------------------------------
# Use this for official releases or CI artifacts
type = url
url = https://pub.cellframe.net/linux/cellframe-node/master/cellframe-node-5.6-master-amd64.deb

# Optional: Verify package integrity with checksum
# checksum = sha256:abc123def456...

# Optional: Custom HTTP headers (for authentication, etc.)
# headers = Authorization: Bearer TOKEN

# ----------------------------------------------------------------------------
# Option 2: Local - Use local .deb package
# ----------------------------------------------------------------------------
# Use this for development with locally built packages
# Path is relative to stage-env/ directory (BASE_PATH)
#
# type = local
# local_path = ../build/cellframe-node-5.6-LOCALBUILD-dbg-amd64.deb
#
# Examples of relative paths from stage-env/:
#   ../build/cellframe-node.deb              - from project build/ dir
#   ../../packages/cellframe-node.deb        - from parent directory
#   /tmp/cellframe-node.deb                  - absolute path
#   ~/Downloads/cellframe-node.deb           - home directory

# ----------------------------------------------------------------------------
# Option 3: Repository - Build from source (git)
# ----------------------------------------------------------------------------
# Use this to build from specific git branch or commit
# This will clone repo and build inside Docker
#
# type = repository
# git_url = https://gitlab.demlabs.net/cellframe/cellframe-node.git
# branch = master                    # or: dev, feature/xyz
# commit = abc123def                 # optional: specific commit hash
#
# Optional: Submodules
# submodules = true                  # clone with --recursive
#
# Optional: Build configuration
# build_type = Release               # or: Debug, RelWithDebInfo
# cmake_flags = -DBUILD_TESTS=ON -DBUILD_DOCS=OFF
#
# Optional: Git credentials for private repos
# git_username = myuser
# git_token = ghp_abc123...          # GitHub Personal Access Token
#
# Optional: Custom build script (instead of default CMake)
# build_script = scripts/custom-build.sh

# ----------------------------------------------------------------------------
# Advanced Options (apply to all types)
# ----------------------------------------------------------------------------

# Cache downloaded/built packages to speed up subsequent runs
# cache_packages = true
# cache_dir = ../testing/cache/packages

# Verify package signature (for official releases)
# verify_signature = true
# gpg_key_url = https://pub.cellframe.net/linux/gpg-key.asc

# Custom Docker build args (passed to docker build)
# docker_build_args = HTTP_PROXY=http://proxy:8080

# Package installation options
# install_recommends = false         # apt-get --no-install-recommends
# force_reinstall = false            # always reinstall even if cached

# ----------------------------------------------------------------------------
# Examples
# ----------------------------------------------------------------------------

# Example 1: Official master build
# type = url
# url = https://pub.cellframe.net/linux/cellframe-node/master/cellframe-node-5.6-master-amd64.deb

# Example 2: Official stable release
# type = url  
# url = https://debian.pub.demlabs.net/public/cellframe-node-5.5-amd64.deb
# checksum = sha256:1234567890abcdef...

# Example 3: Local debug build
# type = local
# local_path = ../build/cellframe-node-5.6-LOCALBUILD-dbg-amd64.deb

# Example 4: Build from dev branch
# type = repository
# git_url = https://gitlab.demlabs.net/cellframe/cellframe-node.git
# branch = dev
# build_type = Debug
# submodules = true

# Example 5: Build from specific commit
# type = repository
# git_url = https://gitlab.demlabs.net/cellframe/cellframe-node.git
# commit = abc123def456
# build_type = RelWithDebInfo

# Example 6: Private repo with authentication
# type = repository
# git_url = https://github.com/myorg/cellframe-fork.git
# branch = feature/my-changes
# git_username = myuser
# git_token = ghp_xxxxxxxxxxxx

# Example 7: CI artifact from URL with auth
# type = url
# url = https://ci.example.com/artifacts/cellframe-node-latest.deb
# headers = Authorization: Bearer CI_TOKEN

# Example 8: Local build with custom path
# type = local
# local_path = /opt/cellframe-builds/cellframe-node-custom.deb

# ============================================================================
# Docker Image Customization (Optional)
# ============================================================================
# Uncomment and configure to customize Docker images for nodes

# [docker_customization]
# # Base packages to install on all nodes (comma-separated)
# # Example packages for debugging and monitoring:
# base_packages = vim,curl,htop,net-tools,strace,tcpdump,iperf3
# 
# # Script to run after base package installation
# # Path is relative to stage-env/ directory
# base_post_script = scripts/base-init.sh

# [role_customization_root]
# # Additional packages for root nodes only
# # Root nodes are seed/bootstrap nodes that other nodes connect to
# packages = prometheus-node-exporter,tcpdump
# 
# # Post-installation script for root nodes
# # Use this to configure monitoring, logging, or special network setup
# post_script = scripts/root-init.sh

# [role_customization_master]
# # Additional packages for master/validator nodes only
# # Master nodes participate in consensus and block validation
# packages = gdb,valgrind,perf
# 
# # Post-installation script for master nodes
# # Use this to configure consensus monitoring or debugging tools
# post_script = scripts/master-init.sh

# [role_customization_full]
# # Additional packages for full (regular) nodes only
# # Full nodes sync the chain but don't participate in consensus
# packages = iperf3,nload
# 
# # Post-installation script for full nodes
# post_script = scripts/full-init.sh

# [node_customization_node1]
# # Specific customization for individual node (node1 in this example)
# # Use this for debugging specific scenarios or testing tools
# packages = gdb,valgrind,strace,ltrace
# 
# # Direct .deb package URLs to install (comma-separated)
# # Useful for installing custom builds or proprietary tools
# deb_urls = https://example.com/custom-tool_1.0_amd64.deb,https://example.com/debug-lib_2.0_amd64.deb
# 
# # Node-specific initialization script
# # This runs last, after all other customizations
# post_script = scripts/node1-init.sh

# [node_customization_node2]
# # Example: Configure node2 for performance profiling
# packages = perf-tools-unstable,linux-tools-generic,sysstat
# post_script = scripts/node2-profiling.sh

# ============================================================================
# Custom DEB Repositories (Optional)
# ============================================================================
# Uncomment to add custom APT repositories to all nodes
# Multiple repositories can be added by creating sections with unique names

# [repository_custom1]
# # Human-readable name for the repository
# name = Custom Development Repository
# 
# # APT repository URL (should include protocol)
# url = https://repo.example.com/ubuntu
# 
# # GPG key URL for repository verification
# # Key will be downloaded and added to APT keyring
# key_url = https://repo.example.com/key.gpg
# 
# # Repository components (space-separated)
# # Common: main, restricted, universe, multiverse
# components = main restricted
# 
# # Distribution codename (e.g., jammy, focal, bionic)
# # Should match your base Docker image Ubuntu version
# distribution = jammy

# [repository_debug]
# # Example: Add Ubuntu debug symbol repository
# name = Ubuntu Debug Symbols Repository
# url = http://ddebs.ubuntu.com
# key_url = https://ddebs.ubuntu.com/dbgsym-release-key.asc
# components = main restricted universe multiverse
# distribution = jammy

# [repository_backports]
# # Example: Add Ubuntu backports for newer packages
# name = Ubuntu Backports
# url = http://archive.ubuntu.com/ubuntu
# # For official Ubuntu repos, key is already trusted
# # key_url =
# components = jammy-backports main restricted universe multiverse
# distribution = jammy

# ============================================================================
# Configuration Notes
# ============================================================================
# 
# Customization Priority (applied in order):
# 1. Base Docker image setup
# 2. [docker_customization] - base packages and scripts for ALL nodes
# 3. [repository_*] - custom APT repositories added
# 4. [role_customization_*] - packages and scripts for specific ROLE
# 5. [node_customization_*] - packages and scripts for specific NODE
# 
# Script Execution:
# - All scripts should be executable (chmod +x)
# - Scripts receive environment variables: NODE_ID, NODE_ROLE, NETWORK_NAME
# - Script exit code 0 = success, non-zero = failure (stops container build)
# - Scripts run as root inside container
# 
# Package Installation:
# - Packages installed via apt-get with --no-install-recommends
# - Installation failures stop container build
# - Use packages available in Ubuntu 22.04 (Jammy) repositories
# 
# Best Practices:
# - Keep customizations minimal for faster builds
# - Test customizations on single node before applying to all
# - Use role-based customization for common patterns
# - Use node-specific customization only when absolutely necessary
# - Document your customizations in comments above each section
# 

# ============================================================================
# Docker Advanced Customization (Per-Node/Per-Role)
# ============================================================================
#
# These settings allow deep Docker customization beyond packages/scripts.
# Can be applied via:
#   1. stage-env.cfg (this file) - for role-based defaults
#   2. YAML scenarios - for specific test requirements
#
# Use cases:
#   - VPN testing: enable TUN/TAP driver (NET_ADMIN capability)
#   - Network simulation: custom volumes for tc/netem configs
#   - Hardware access: mount specific devices (GPU, USB, serial ports)
#   - Advanced networking: custom network modes, sysctls
#
# ============================================================================

# Example configurations (commented out by default):

# [role_docker_root]
# # Docker customizations for ALL root nodes
# 
# # Additional volume mounts (host:container[:mode])
# # volumes = /dev/shm:/dev/shm:rw
# #          /host/vpn/configs:/etc/vpn:ro
# 
# # Additional Linux capabilities
# # capabilities = NET_ADMIN,NET_RAW,SYS_ADMIN
# 
# # Device mappings (for hardware access - GPU, USB, etc)
# # devices = /dev/nvidia0,/dev/nvidiactl
# 
# # Raw docker-compose config sections (advanced)
# # Merged into generated docker-compose.yml
# # extra_config = {"sysctls": {"net.ipv4.ip_forward": "1"},
# #                 "privileged": false}

# [node_docker_1]
# # Docker customizations for specific node (node1)
# # Useful for asymmetric testing scenarios
# 
# volumes = /tmp/node1-data:/opt/test-data:rw
# capabilities = NET_ADMIN,NET_RAW
# 
# # Example: Enable IP forwarding for VPN/routing tests
# # extra_config = {"sysctls": {"net.ipv4.ip_forward": "1"}}

# ============================================================================
# Configuration Priority for Docker Customizations
# ============================================================================
#
# Docker settings are applied in this order (later overrides earlier):
#
# 1. Base docker-compose.yml template (standard volumes, network, etc)
# 2. [role_docker_<role>] - role-based docker customizations
# 3. Per-node from YAML scenario - scenario-specific requirements
# 4. [node_docker_<id>] - node-specific overrides (highest priority)
#
# Merging rules:
#   - volumes:      Lists are EXTENDED (all volumes kept)
#   - capabilities: Lists are EXTENDED (all caps added)
#   - devices:      Lists are EXTENDED (all devices added)
#   - environment:  Dicts are MERGED (later values win)
#   - extra_config: Deep merge (dicts merged, lists extended, scalars replaced)
#
# ============================================================================

# ============================================================================
# Common Use Cases
# ============================================================================

# Use Case 1: VPN/TUN Testing (OpenVPN, WireGuard inside container)
# -------------------------------------------------------------------
# [role_docker_full]
# # Enable TUN/TAP driver inside container (usually already present)
# # Only NET_ADMIN capability needed, /dev/net/tun typically exists by default
# capabilities = NET_ADMIN,NET_RAW
# extra_config = {"sysctls": {"net.ipv4.ip_forward": "1",
#                             "net.ipv6.conf.all.disable_ipv6": "0"}}

# Use Case 2: Shared Memory for IPC
# ----------------------------------
# [docker_customization]
# volumes = /dev/shm:/dev/shm:rw

# Use Case 3: Debug with GDB/Strace
# ----------------------------------
# [node_docker_1]
# capabilities = SYS_PTRACE
# extra_config = {"security_opt": ["apparmor=unconfined"]}

# Use Case 4: Custom Network Tools
# ---------------------------------
# [role_customization_master]
# packages = iproute2,tcpdump,iperf3,netcat-openbsd,wireguard-tools,openvpn

# Use Case 5: Persistent Test Data
# ---------------------------------
# [node_docker_1]
# volumes = /host/persistent/node1:/opt/cellframe-node/var/test-data:rw

# Use Case 6: Network Simulation with TC/Netem
# ----------------------------------------------
# [role_docker_master]
# capabilities = NET_ADMIN
# extra_config = {"sysctls": {"net.core.rmem_max": "2500000",
#                             "net.core.wmem_max": "2500000"}}
#
# [role_customization_master]
# packages = iproute2
# post_script = /path/to/setup-netem.sh

# Use Case 7: GPU Access (for crypto acceleration)
# -------------------------------------------------
# [node_docker_1]
# devices = /dev/nvidia0,/dev/nvidiactl,/dev/nvidia-uvm
# extra_config = {"runtime": "nvidia"}
#

[suite_isolation]
# ============================================================================
# Suite Isolation via Snapshots
# ============================================================================
# Speed up tests by restoring clean environment state from snapshots
# instead of slow cleanup/recreation between test suites
#
# Available modes:
#   - disabled    : No snapshots, use current cleanup logic
#   - recreate    : Full cleanup and rebuild (~40s, baseline)
#   - filesystem  : Directory copy/restore with rsync (~3s, balanced)
#   - squashfs    : Read-only image with overlay (~2s, maximum speed)
#
# Performance comparison:
#   Mode        | Creation | Restore | Isolation | Storage
#   ------------|----------|---------|-----------|----------
#   disabled    | N/A      | N/A     | Current   | None
#   recreate    | 0s       | ~40s    | Maximum   | Minimal
#   filesystem  | ~3s      | ~3s     | High      | ~500MB
#   squashfs    | ~5s      | ~2s     | High      | ~100MB (compressed)
# ============================================================================

# Snapshot mode selection
mode = filesystem

# Squashfs compression (only for squashfs mode)
# Options: none, gzip, lzo, xz
# - none: Fastest creation/restore, larger files (~500MB)
# - gzip: Balanced, good compression (~150MB)
# - lzo: Fast compression, moderate size (~200MB)
# - xz: Best compression, slower creation (~100MB)
squashfs_compression = none

# Automatic snapshot management
# Create initial snapshot after network startup
auto_create_on_startup = true

# Snapshot name to use for suite isolation
snapshot_name = clean_state

# Cleanup old snapshots (keep only N most recent)
auto_cleanup = true
keep_snapshot_count = 5

# Snapshot storage directory (relative to stage-env/)
snapshots_dir = ../testing/snapshots
